---
title: Оценка приблизительного числа уникальных элементов в наборе данных
description: Применения алгоритма HyperLogLog для приблизительного вычисления числа уникальных элементов в наборе данных
tags: cardinality-estimation, big-data, LogLog, SuperLogLog, HyperLogLog
---

Предположим, что у нас имеется очень большой набор данных с дубликатами, который не помещается в оперативную память. 
Нам необходимо подсчитать число уникальных элементов в этом наборе. Можно было бы, конечно, отсортировать данные на диске и подсчитать точно,
но мы этого делать не хотим, нам бы хотелось подсчитать это хотя бы приблизительно за один проход. В таком случае мы сможем даже на живом потоке 
данных в режиме реального времени вычислять число полученных уникальных элементов, например, число уникальных IP адресов.

## Простой интуитивный подход

Для начала упростим нашу задачу и предположим, что наши данные - это равномерно распределенные в заранее известном интервале целые числа. Попробуем 
сгенерировать несколько наборов данных с одинаковым числом уникальных случайных чисел в каждом наборе. Пусть в каждом наборе будет 7 уникальных
случайных чисел равномерно распределенных в интервале [0, 65535] (16 бит), сгенерируем 20 таких наборов:

|       |1|2|3|4|5|6|7|
|:-----:|------:|------:|------:|------:|------:|------:|------:|
|**1**|13502|36579|36627|47595|50333|53564|59377|
|**2**|17937|19130|28978|30625|30769|41461|64064|
|**3**|1990|20617|44379|53536|56516|64366|65231|
|**4**|2138|34490|45063|45871|55577|61211|65005|
|**5**|5315|12263|19475|29709|35836|42119|64785|
|**6**|1991|22488|24969|33104|46921|50001|62747|
|**7**|7796|9622|17920|22096|29963|52085|59447|
|**8**|12773|14504|26874|31979|40621|47102|57553|
|**9**|14313|40951|44548|48195|54094|56244|58678|
|**10**|5674|11150|14364|42212|54186|54957|58749|
|**11**|6614|31546|42742|47167|47781|53048|65534|
|**12**|8226|12394|21332|31248|35441|47084|59321|
|**13**|30696|43977|50102|52011|53031|54624|61442|
|**14**|14582|18790|19332|21796|34261|43476|53779|
|**15**|4490|12246|14505|40935|41313|45419|60793|
|**16**|4359|6158|7010|9771|44466|58989|60863|
|**17**|9703|14772|32153|37346|39968|44407|62458|
|**18**|333|4083|18458|23654|40513|46201|55737|
|**19**|18371|25371|33198|34258|34837|62821|64802|
|**20**|3736|6826|18386|32511|40183|48649|55470|
|**avg**|9226.95|19897.85|28020.75|35780.95|43330.50|51391.40|60791.75|

В последней строке мы вычислили среднее значение в каждом столбике. Посмотрим на поведение чисел в каждом столбике.
От набора к набору числа в отдельно взятом столбце менялись довольно сильно, но их среднее в конце-концов сходится ко вполне конкретному числу.
Средние значения по столбцу имеют тенденцию с одинаковым расстоянием друг от друга заполнять весь интервал [0, 65535]. 
Как несложно заметить, среднее значение в первом столбце равно примерно шагу разбиения интервала [0, 65535] на равные части. Таким образом,
если бы кто-то сгенерировал похожую таблицу и сказал бы нам среднее значение первого столбца, то мы бы могли оценить число уникальных
элементов, которое было в каждом наборе. Для этого мы бы просто длину интервала (65535) поделили бы на шаг разбиения (среднее
значение первого столбца равное 9226.95) и отняли единицу (потому что если колбасу разрезать на две равных части, 
разрез будет один, а не два). В нашем случае получается оценка в \\( \\frac { 65535 }{ 9226.95 } - 1 = 6.01 \\) уникальных 
элементов в каждой выборке. Понятно, что чем больше выборок, тем точнее будет оценка.

Можно так же сказать иначе, что среднее значение первого столбца есть ни что иное как среднее значение минимальных элементов из каждой выборки. Т.е. мы
просто берем минимальные элементы из каждой выборки и находим их среднее. Таким образом нам не нужно даже запоминать все элементы каждой выборки, чтобы оценить число
уникальных элементов, достаточно где-то хранить минимальные значения каждой из выборок.

## Бит-паттерны

Представим наши случайные данные в виде двоичных чисел. Чем меньше число, тем больше в его двоичном представлении старших битов установлено в ноль:

| десятичное представление | двоичное представление |
|:------------------------:|:----------------------:|
|0|**000**|
|1|**00**1|
|2|**0**10|
|3|**0**11|
|4|100|
|5|101|
|6|110|
|7|111|

Поэтому, вместо того, чтобы определять минимальное число в выборке можно определять в выборке максимальное число старших битов установленных в ноль.
Если мы случайным образом генерируем число из n бит, то вероятность того, что k старших битов будет установлено в ноль, равна:

$$ \underbrace { \frac { 1 }{ 2 } \times \cdots \times \frac { 1 }{ 2 }  }_{ k } =\frac { 1 }{ { 2 }^{ k } } $$

Т.е. в среднем каждые \\( { 2 }^{ k } \\) раз старшие k битов будут установлены в ноль. Это позволяет нам грубо с точностью до степени двойки
определять число уникальных элементов в выборке.

Преобразуем таблицу со случайными выборками так, чтобы она содержала число старших битов установленных в ноль для каждого случайного числа
и для каждой выборки определим максимальное число старших битов установленных в ноль:

|       |1|2|3|4|5|6|7|макс. число старших битов установленных в ноль|
|:-----:|------:|------:|------:|------:|------:|------:|------:|:----------------:|
|**1**|(13502 = b**00**11010010111110) 2|0|0|0|0|0|0|2|
|**2**|(17937 = b**0**100011000010001) 1|1|1|1|1|0|0|1|
|**3**|(1990 = b**00000**11111000110) 5|1|0|0|0|0|0|5|
|**4**|(2138 = b**0000**100001011010) 4|0|0|0|0|0|0|4|
|**5**|(5315 = b**000**1010011000011) 3|2|1|1|0|0|0|3|
|**6**|(1991 = b**00000**11111000111) 5|1|1|0|0|0|0|5|
|**7**|(7796 = b**000**1111001110100) 3|2|1|1|1|0|0|3|
|**8**|(12773 = b**00**11000111100101) 2|2|1|1|0|0|0|2|
|**9**|(14313 = b**00**11011111101001) 2|0|0|0|0|0|0|2|
|**10**|(5674 = b**000**1011000101010) 3|2|2|0|0|0|0|3|
|**11**|(6614 = b**000**1100111010110) 3|1|0|0|0|0|0|3|
|**12**|(8226 = b**00**10000000100010) 2|2|1|1|0|0|0|2|
|**13**|(30696 = b**0**111011111101000) 1|0|0|0|0|0|0|1|
|**14**|(14582 = b**00**11100011110110) 2|1|1|1|0|0|0|2|
|**15**|(4490 = b**000**1000110001010) 3|2|2|0|0|0|0|3|
|**16**|(4359 = b**000**1000100000111) 3|3|3|2|0|0|0|3|
|**17**|(9703 = b**00**10010111100111) 2|2|1|0|0|0|0|2|
|**18**|(333 = b**0000000**101001101) 7|4|1|1|0|0|0|7|
|**19**|(18371 = b**0**100011111000011) 1|1|0|0|0|0|0|1|
|**20**|(3736 = b**0000**111010011000) 4|3|1|1|0|0|0|4|
|**avg**| | | | | | | |2.9|

В среднем 2.9 битов установлено в ноль, что позволяет грубо определить количество уникальных элементов в каждой выборке:

$$ { 2 }^{ 2.9 }=7.46 $$

Эта оценка является смещенной и требует дополнительного умножения на "константу Чапаева", которая зависит от числа выборок.

Таким образом, вместо минимальных чисел в выборке, мы используем максимальное число старших битов установленных в ноль. Это позволит
нам в дальнейшем сэкономить на памяти, т.к., например, вместо того, чтобы хранить 32-битное минимальное число, мы будем хранить максимальное
число старших битов установленных в ноль, для чего достаточно 5 бит (\\( 32 = { 2 }^{ 5 } \\)). На самом деле, как несложно заметить, вместо
подсчета числа старших битов установленных в ноль, можно считать число младших битов установленных в ноль - вероятности получаются одинаковыми.

Что нам это дает? Ведь в исходной постановке задачи у нас есть всего одна выборка, хоть и очень большая, к тому же данные у нас скорее всего
распределены не равномерно, хотя мы и можем сказать, каким числом данные ограничены сверху. Например, если говорить об IP адресах, то понятно,
что нам достаточно 4-х байтов или 32 битов, чтобы представить все возможные значения адресов, однако IP адреса по конкретному региону 
будут скорее всего сосредоточены в каком-то узком диапазоне этих значений и никак не будут распределены равномерно по всему диапазону.

## Хеширование и стохастическое усреднение

Хоть у нас и есть одна большая выборка с неслучайными данными, возникает естественное желание создать искусственное отображение этих неслучайных данных в
несколько (чем больше, тем лучше) выборок со случайными данными. Если говорить точнее, то мы хотим придумать такие хеш-функции 
$$ { f }_{ 1 }, { f }_{ 2 }, \dots , { f }_{ s } $$
каждая из которых отображала бы первоначальную выборку с неслучайными данными в выборку со случайными данными, равномерно распределенными на каком-то интервале
фиксированной длины (важно, чтобы значения всех функций были в одном интервале, чтобы потом ничего не нормировать). Таким образом у нас бы было s выборок со 
случайными данными, с примерно одинаковым числом случайных данных (в виду возможных коллизий) в каждой выборке и мы смогли бы тогда легко оценить число уникальных
элементов, как мы это делали выше.

То, что мы хотим сделать, *теоретически* невозможно, но Дональд Кнут в третьем томе своей книги "Искусство программирования" (Вильямс, 2000) нас обнадеживает:

> *Теоретически невозможно определить хеш-функцию так, чтобы она создавала случайные данные из реальных неслучайных файлов. Однако на практике реально создать
> достаточно хорошую имитацию с помощью простых арифметических действий. Более того, зачастую можно использовать особенности данных для создания хеш-функций с
> минимальным числом коллизий (меньшим, чем при истинно случайных данных).*

При наличии s таких хеш-функций, мы бы могли завести массив на s элементов (по элементу на каждую хеш-функцию), где каждый элемент массива \\( r_i \\) будет представлять
максимальное число старших битов установленных в ноль среди полученных хеш-значений данной хеш-функции. Тогда число уникальных элементов можно было бы оценить, как

$$ constant\cdot { 2 }^{ \overline { R }  } $$

где 

$$ \overline { R } =\frac { 1 }{ s } \sum _{ i=1 }^{ s }{ { r }_{ i } } $$

Хотя интуитивная идея подсчета числа уникальных элементов уже несколько оформилась на бумаге, но она не достаточно хороша, потому что нам, во-первых, нужно еще придумать
s разных независимых друг от друга хеш-функций, причем так, чтобы мы могли легко изменять их число в зависимости от желаемой точности, во-вторых, вычисление s хешей для
каждого элемента может быть несколько накладным. Вместо этого прибегнем к небольшому трюку - будем использовать всего одну хеш-функцию, старшие k бит которой 
будут определять номер выборки, а оставшиеся биты будут хеш значением, которое должно попасть в ту выборку. Эта процедура называется [стохастическим усреднением](http://www.mathcs.emory.edu/~cheung/papers/StreamDB/Probab/1985-Flajolet-Probabilistic-counting.pdf). 

Например, выберем k=4 (т.е. результирующих выборок будет \\( m={ 2 }^{ 4 }=16 \\)), и для какого-то элемента выборки, хеш-значение
оказывается равным 1011100110101011 (16-битная хеш-функция, n=16) в двоичном представлении:
$$ \underbrace { 1011 }_{ k=4 } \underbrace { 100110101011 }_{ n-k=12 } $$
Это будет означать, что мы сгенерировали 12-битный хеш b100110101011=2475, который попадает только в выборку b1011=11, если нумеровать выборки с нуля.

Таким образом каждый элемент исходной выборки попадает случайным образом только в одну из m выборок, в то же время все m выборок, в виду случайности распределения,
заполняются равномерно примерно одинаковым числом элементов, поэтому прежнюю оценку числа уникальных элементов необходимо умножить на m, чтобы оценить не просто число
уникальных элементов в каждой отдельной выборке, а в целом, сколько было в первоначальной:

$$ constant\cdot m\cdot { 2 }^{ \overline { R }  }=constant\cdot { 2 }^{ k }\cdot { 2 }^{ \overline { R }  } $$

## Алгоритм LogLog

Последняя формула, которую мы получили выше, является оценочной функцией в алгоритме [LogLog](http://algo.inria.fr/flajolet/Publications/DuFl03-LNCS.pdf).
Единственным отличием является то, что вместо числа нулей в старших битах, там вычисляется позиция \\( \\rho (x) \\) первого ненулевого старшего бита, т.е. на
единицу большее значение, так что \\( \\rho (1\dots )=1 \\), \\( \\rho (001\dots )=3 \\) и т.д.
Ну и, соответственно, "константа Чапаева" немного меняется в зависимости от числа выборок. Весь алгоритм выглядит так:

1. Выбираем число k такое, что \\( m={ 2 }^{ k } \\) будет числом выборок, между которыми распределяться все хеш-значения. От этого числа будет зависеть точность оценки.
2. Инициализируем массив из m элементов нулями: \\( { M }^{ (0) },\\dots ,{ M }^{ (m-1) } \\)
3. Для каждого каждого элемента первоначальной выборки вычисляем хеш (в двоичном виде):
   $$ x={ b }_{ 1 }{ b }_{ 2 }\dots $$ 
   определяем номер выборки, как значение первых k бит в системе счисления с основанием 2:
   $$ j:={ \left< { b }_{ 1 }\dots { b }_{ k } \right>  }_{ 2 } $$ 
   и обновляем соответствующий элемент массива:
   $$ { M }^{ (j) }:=\max { \left( { M }^{ (j) },\rho ({ b }_{ k+1 }{ b }_{ k+2 }\dots ) \right)  } $$
4. Вычисляем оценку числа уникальных элементов как
   $$  E:={ \alpha  }_{ m }\cdot m\cdot { 2 }^{ \frac { 1 }{ m } \sum _{ j }^{  }{ { M }^{ (j) } }  } $$
   $$ { \alpha  }_{ m }\sim { \alpha  }_{ \infty  }-\frac { 2{ \pi  }^{ 2 }+{ \ln^{2}{2}  } }{ 48m } $$
   $$ { \alpha  }_{ \infty  }={ e }^{ -\gamma  }\frac { \sqrt { 2 }  }{ 2 } \doteq 0.397011808 $$
   где \\( \\gamma \\) - [постоянная Эйлера](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%BD%D0%B0%D1%8F_%D0%AD%D0%B9%D0%BB%D0%B5%D1%80%D0%B0_%E2%80%94_%D0%9C%D0%B0%D1%81%D0%BA%D0%B5%D1%80%D0%BE%D0%BD%D0%B8).
   На практике константа \\( \\alpha_m \\) может быть заменена на \\( \\alpha_\\infty \\) при \\( m\\ge64 \\).
   
Стандартная ошибка оценки по алгоритму LogLog составляет \\( \\approx \\frac { 1.30 }{ \\sqrt { m }  } \\). Таким образом при m=256 и m=1024 стандартная ошибка 
составляет соответственно 8.1% и 4.1% от истинного числа уникальных элементов. Как мы помним, нам достаточно 5 бит для каждого из m элементов массива, поэтому при
m=1024 используемая память будет составлять 5120 бит или 640 байт, что довольно неплохо, для стандартной ошибки в 4.1%.

## Увеличиваем точность: алгоритмы SuperLogLog и HyperLogLog

Ученые Durand и Flajolet пришли к выводу, что можно еще улучшить алгоритм, если перед усреднением выбросить 30% элементов с наибольшими значениями и 
усреднить оставшиеся 70% значений, в таком случае стандартная ошибка оценки уменьшается с \\( \\frac { 1.30 }{ \\sqrt { m }  } \\) до \\( \\frac { 1.05 }{ \\sqrt { m }  } \\).
Это значит, что при m=1024 и используемой при этом памяти 640 байт стандартная ошибка уменьшается с 4.1% до 3.28%. Так появился алгоритм SuperLogLog.

Следующим значительным вкладом Flajolet было использование гармонического среднего, в результате чего возник алгоритм [HyperLogLog](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf),
в котором стандартную ошибку оценки удалось уменьшить до \\( \\frac { 1.04 }{ \\sqrt { m }  } \\).



## Производим параллельные вычисления